{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Readability Exercise\n",
    "\n",
    "Welcome! Below you will implement two metrics for evaluating the readability of documents:\n",
    "\n",
    "1. Flesch–Kincaid readability Grade Index \n",
    "2. Gunning Fog Index\n",
    "\n",
    "The solutions are in [readability_solutions.py](./readability_solutions.py). You can also click the jupyter icon to see all the files in the folder.\n",
    "\n",
    "To load all the functions in the solutions, simply include `from solutions import *`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Initialization\n",
    "\n",
    "Let's read-in our text files. We have three different texts files to play with: \n",
    "\n",
    "1. `physics.txt`: taken from a technical wikipedia article about a theoretical physics idea called [Supersymmetry](https://en.wikipedia.org/wiki/Supersymmetry)\n",
    "\n",
    "2. `APPL_10k_2017.txt`: the 2017 10-K Item IA for APPLE INC, taken from the EDGAR website\n",
    "\n",
    "3. `alice.txt`: Excerpts from \"Alice in Wonderland\", the novel is in the public domain and freely available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading 10-Ks item 1A for CIK = 0001065088 ...\n",
      "skipping EBAY_10k_2017.txt\n",
      "skipping EBAY_10k_2016.txt\n",
      "skipping EBAY_10k_2015.txt\n",
      "skipping EBAY_10k_2014.txt\n",
      "skipping EBAY_10k_2013.txt\n",
      "downloading 10-Ks item 1A for CIK = 0000320193 ...\n",
      "skipping AAPL_10k_2017.txt\n",
      "skipping AAPL_10k_2016.txt\n",
      "skipping AAPL_10k_2015.txt\n",
      "skipping AAPL_10k_2014.txt\n",
      "skipping AAPL_10k_2013.txt\n",
      "downloading 10-Ks item 1A for CIK = 0001310067 ...\n",
      "skipping SHLDQ_10k_2017.txt\n",
      "skipping SHLDQ_10k_2016.txt\n",
      "skipping SHLDQ_10k_2015.txt\n",
      "skipping SHLDQ_10k_2014.txt\n",
      "skipping SHLDQ_10k_2013.txt\n"
     ]
    }
   ],
   "source": [
    "# download some excerpts from 10-K files\n",
    "\n",
    "from download10k import get_files\n",
    "\n",
    "CIK = {'ebay': '0001065088', 'apple':'0000320193', 'sears': '0001310067'}\n",
    "get_files(CIK['ebay'], 'EBAY')\n",
    "get_files(CIK['apple'], 'AAPL')\n",
    "get_files(CIK['sears'], 'SHLDQ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentences separated by ; are better viewed as multiple sentences\n",
    "# join combines all the newlines in the file\n",
    "\n",
    "f = open(\"physics.txt\", \"r\")\n",
    "text_phy=''.join(f).replace(';','.')\n",
    "\n",
    "f = open(\"alice.txt\", \"r\")\n",
    "text_alice=''.join(f).replace(';','.')\n",
    "\n",
    "f = open(\"AAPL_10k_2017.txt\", \"r\")\n",
    "text_10k=''.join(f).replace(';','.')\n",
    "\n",
    "f = open(\"chinese_virus.txt\", \"r\")\n",
    "text_chinese_virus=''.join(f).replace(';','')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following discussion of risk factors contains forward-looking statements. These risk factors may be important to understanding other statements in this Form 10-K. The following information should be read in conjunction with Part II, Item 7, “Management’s Discussion and Analysis of Financial Condition and Results of Operations” and the consolidated financial statements and related notes in Part II, Item 8, “Financial Statements and Supplementary Data” of this Form 10-K. The business, financia...\n",
      "\n",
      "In particle physics, supersymmetry (SUSY) is a principle that proposes a relationship between two basic classes of elementary particles: bosons, which have an integer-valued spin, and fermions, which have a half-integer spin. A type of spacetime symmetry, supersymmetry is a possible candidate for undiscovered particle physics, and seen as an elegant solution to many current problems in particle physics if confirmed correct, which could resolve various areas where current theories are believed to...\n",
      "\n",
      "Alice was beginning to get very tired of sitting by her sister on the bank, and of having nothing to do: once or twice she had peeped into the book her sister was reading, but it had no pictures or conversations in it, “and what is the use of a book,” thought Alice, “without pictures or conversations?”. So she was considering, in her own mind (as well as she could, for the hot day made her feel very sleepy and stupid), whether the pleasure of making a daisy-chain would be worth the trouble of ge...\n",
      "\n",
      "Paying to stop the pandemic\n",
      "The struggle to save lives and the economy is likely to present agonising choices\n",
      "\n",
      "Leaders\n",
      "Mar 19th 2020 edition\n",
      "Mar 19th 2020\n",
      "Editor’s note: The Economist is making some of its most important coverage of the covid-19 pandemic freely available to readers of The Economist Today, our daily newsletter. To receive it, register here.\n",
      "\n",
      "Planet earth is shutting down. In the struggle to get a grip on covid-19, one country after another is demanding that its citizens shun soci...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check out some of the texts\n",
    "print(text_10k[:500]+\"...\\n\")\n",
    "print(text_phy[:500]+\"...\\n\")\n",
    "print(text_alice[:500]+\"...\\n\")\n",
    "print(text_chinese_virus[:500]+\"...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Pre-processing\n",
    "Here, we need to define functions that can split our texts into sentences, and split our sentences into words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package cmudict to\n",
      "[nltk_data]     /Users/manwaiwong/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/cmudict.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/manwaiwong/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/manwaiwong/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# tokenize and clean the text\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from nltk import word_tokenize\n",
    "from syllable_count import syllable_count\n",
    "\n",
    "\n",
    "nltk.download('wordnet')\n",
    "\n",
    "sno = SnowballStemmer('english')\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "# tokenizer that selects out non letter and non symbol (i.e. all alphabets)\n",
    "word_tokenizer = RegexpTokenizer(r'[^\\d\\W]+')\n",
    "\n",
    "\n",
    "def word_tokenize(sent):\n",
    "    return [ w for w in word_tokenizer.tokenize(sent) if w.isalpha() ]\n",
    "\n",
    "# for the sentence tokenizer\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# you can tokenize sentences by calling\n",
    "# sent_tokenize(document)\n",
    "\n",
    "# and tokenize words by calling\n",
    "# word_tokenize(sentence)\n",
    "\n",
    "# syllable_count counts the number of syllables in a word\n",
    "# it's included in syllable_count.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now implement three functions\n",
    "\n",
    "1. `sentence_count`: a simple function that returns the number of sentences in a document\n",
    "\n",
    "2. `word_count`: a simple function that returns the number of words in a sentence\n",
    "\n",
    "3. `hard_word_count`: a simple function that returns the number of words with more than 3 syllables, while removing suffixes like \"-ed\", and \"-ing\". This can be done by lemmatizing the word, i.e. `wnl.lemmatize(word, pos='v')`\n",
    "\n",
    "the function `word_tokenize(sentence)` will be useful here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_count(text):\n",
    "    #https://www.nltk.org/api/nltk.tokenize.html\n",
    "    return len(sent_tokenizer.tokenize(text))\n",
    "\n",
    "def word_count(sent):\n",
    "    return len([ w for w in word_tokenize(sent)])\n",
    "\n",
    "def hard_word_count(sent):\n",
    "    return len([ w for w in word_tokenize(sent) \\\n",
    "                if syllable_count(wnl.lemmatize(w, pos='v'))>=3 ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Readability Grade-Levels\n",
    "\n",
    "Here, you will implement the two readability indices (grade levels). They are defined by\n",
    "\n",
    "\\begin{align}\n",
    "\\textrm{Flesch–Kincaid Grade} \n",
    "= 0.39 \\left(\n",
    "\\frac{\\textrm{Number of words}}{\\textrm{Number of sentences}}\n",
    "\\right) \\\\\n",
    "+11.8\n",
    "\\left(\n",
    "\\frac{\\textrm{Number of syllables}}{\\textrm{Number of words}}\n",
    "\\right)\n",
    "-15.59\n",
    "\\end{align}\n",
    "\n",
    "and\n",
    "\n",
    "\\begin{align}\n",
    "\\textrm{Gunning-Fog Grade} \n",
    "=\\; &0.4 \\bigg[ \n",
    "\\left(\n",
    "\\frac{\\textrm{Number of words}}{\\textrm{Number of sentences}}\n",
    "\\right) \n",
    "+100\n",
    "\\left(\n",
    "\\frac{\\textrm{Number of hard words}}{\\textrm{Number of words}}\n",
    "\\right)\n",
    "\\bigg]\n",
    "\\end{align}\n",
    "\n",
    "To count syllables, we've added a syllable_count function you can access via \n",
    "\n",
    "```\n",
    "from syllable_count import syllable_count\n",
    "syllable_count(\"syllable\")\n",
    "```\n",
    "\n",
    "Below, implement the function `flesch_index` and `fog_index` that computes the readability grade level for a given text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def flesch_index(text):\n",
    "    sentences = sent_tokenize(text)\n",
    "\n",
    "    total_sentences = len(sentences)\n",
    "    total_words = np.sum([ word_count(s) for s in sentences ])\n",
    "    total_syllables = np.sum([ np.sum([ syllable_count(w) for w in word_tokenize(s) ]) \\\n",
    "                              for s in sentences ])\n",
    "    \n",
    "    return 0.39*(total_words/total_sentences) + \\\n",
    "            11.8*(total_syllables/total_words) - 15.59\n",
    "\n",
    "def fog_index(text):\n",
    "    sentences = sent_tokenize(text)\n",
    "\n",
    "    total_sentences = len(sentences)\n",
    "    total_words = np.sum([ word_count(s) for s in sentences ])\n",
    "    total_hard_words = np.sum([ hard_word_count(s) for s in sentences ])\n",
    "    \n",
    "    return 0.4*((total_words/total_sentences) + \\\n",
    "            100.0*(total_hard_words/total_words))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Results\n",
    "\n",
    "Now that you've coded up the exercises, compute the grade levels for each of the texts given.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.7809465240641735 9.736541889483068\n",
      "16.317171212337573 19.32253320005712\n",
      "18.210828810597004 21.561449068210177\n",
      "10.07935021155443 12.14425639606153\n"
     ]
    }
   ],
   "source": [
    "# to test the solutions\n",
    "# uncommon next line\n",
    "# from readability_solutions import *\n",
    "\n",
    "print(flesch_index(text_alice),fog_index(text_alice))\n",
    "print(flesch_index(text_phy),fog_index(text_phy))\n",
    "print(flesch_index(text_10k),fog_index(text_10k))\n",
    "print(flesch_index(text_chinese_virus),fog_index(text_chinese_virus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should expect a grade level around 7-10 for `alice.txt`, and around 16-19 for `physics.txt`, and 18+ for financial documents! \n",
    "\n",
    "It turns out 10-Ks are really *hard* to read legal documents!\n",
    "Now, let's compute the readability for all the 10-Ks we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AAPL_10k_2013.txt 18.133659675661395 21.421954178639453\n",
      "AAPL_10k_2014.txt 18.153689466463494 21.533048686029034\n",
      "AAPL_10k_2015.txt 18.21447063787632 21.60600512453601\n",
      "AAPL_10k_2016.txt 18.26201968934649 21.636142401295338\n",
      "AAPL_10k_2017.txt 18.210828810597004 21.561449068210177\n",
      "EBAY_10k_2013.txt 17.208826114904138 19.467371718858438\n",
      "EBAY_10k_2014.txt 17.522305956953222 19.84433209495174\n",
      "EBAY_10k_2015.txt 17.174143846853443 19.517270443530904\n",
      "EBAY_10k_2016.txt 16.8119978035587 19.212192585842303\n",
      "EBAY_10k_2017.txt 16.98803671402523 19.398021171433715\n",
      "SHLDQ_10k_2013.txt 16.81263051159367 19.21544203165256\n",
      "SHLDQ_10k_2014.txt 17.11381269946128 19.525376592201123\n",
      "SHLDQ_10k_2015.txt 18.30411852697303 21.001601156700893\n",
      "SHLDQ_10k_2016.txt 18.732102085355468 21.478160676371758\n",
      "SHLDQ_10k_2017.txt 17.755571973024427 20.645205784832935\n"
     ]
    }
   ],
   "source": [
    "filelist_10k=!ls *10k*txt\n",
    "\n",
    "\n",
    "flesch = []\n",
    "fog = []\n",
    "\n",
    "for file in filelist_10k:\n",
    "    with open(file, 'r') as f:\n",
    "        text=''.join(f).replace(';','.')\n",
    "        flesch.append(flesch_index(text))\n",
    "        fog.append(fog_index(text))\n",
    "        print(file, flesch[-1],fog[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Superficially, and according to our readability metrics, reading 10-Ks is harder than reading articles on theoretical physics!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus exercise:\n",
    "How are the two readability grade-levels correlated? Compute the covariance matrix of the two readability indices we have on all the 10K documents, and make a scatter plot of Flesch index vs Fog index. Also perform a least-squared fit to the result and plot it as well.\n",
    "\n",
    "(change bottom cell to code and remove html tags for solution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import linregress\n",
    "\n",
    "\n",
    "# TO DO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"white\">\n",
    "\n",
    "#solution\n",
    "cov = np.cov(flesch,fog)\n",
    "print(cov)\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "plt.figure(figsize=(6,5))\n",
    "plt.scatter(flesch,fog) \n",
    "\n",
    "slope, intercept, r_value, p_value, std_err = linregress(flesch, fog)\n",
    "\n",
    "x=np.linspace(16.5,19,101)\n",
    "y=slope*x+intercept\n",
    "plt.plot(x,y)\n",
    "\n",
    "plt.xlabel(\"Flesch Index\")\n",
    "plt.ylabel(\"Fog Index\")\n",
    "\n",
    "</font>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
